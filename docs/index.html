<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Melika Ayoughi</title>

  <meta name="author" content="Melika Ayoughi">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="./stylesheet.css">
	<link rel="icon" href="./images/uva_logo.png">
</head>

<body>
<!--<style>-->
<!--body {-->
<!--  background-color: coral;-->
<!--}-->
<!--</style>-->

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Melika Ayoughi</name>
              </p>
              <p style="text-align:center">
                <a href="mailto:m.ayoughi@uva.nl">Email</a> &nbsp/&nbsp
                <a href="./data/Melika_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/melika-ayoughi/">LinkedIn</a> &nbsp/&nbsp
<!--                <a href="https://scholar.google.com/">Google Scholar</a> &nbsp/&nbsp-->
                <a href="https://twitter.com/melikaayoughi">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Melika-Ayoughi/">Github</a>

              </p>

              <p>I am a PhD student at the <a href="https://www.uva.nl/en">University of Amsterdam</a>, where I work on extracting knowledge graphs from videos. I'm supervised jointly by <a href="https://staff.fnwi.uva.nl/p.s.m.mettes/">Pascal Mettes</a>  from <a href="https://ivi.fnwi.uva.nl/vislab/">VIS Lab</a> and <a href="http://pgroth.com/">Paul Groth</a>  from <a href="https://indelab.org/">INDE Lab</a>.
              </p>
              <p>During my internship at <a href="https://machinelearning.apple.com/">Apple MLR</a>, I worked with <a href="https://scholar.google.com.sg/citations?user=B_5DFeMAAAAJ&hl=en">Hanlin</a> on self-supervised pertaining of transformers using relative patch transformations. Before my PhD, I did an Artificial Intelligence master at the University of Amsterdam. I worked on my thesis at <a href="https://www.tomtom.com/">TomTom</a> on <a href="data/Multi-Scale Gambler for Object Detection.pdf"> object detection under high class imbalance</a>. I took part in an internship at <a href="https://dexterenergy.ai/">Dexter Energy Services</a> working on <a href="data/internship_report.pdf"> weather nowcasting using satellite images</a>.</p>


            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./images/MelikaAyoughi.png"><img style="width:100%;max-width:100%" alt="profile photo" src="./images/MelikaAyoughi.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, and specifically multimodal data such as video, audio, text and graph. I would like to work on improving high-level understanding of videos such as object-centric learning and relationship detection and easing retrieval by storing information in a knowledge graph. I'm currenlty working on a project to learn better instance-level represetations using prior knowledge.
<!--                <span class="highlight">highlighted</span>.-->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <span class="dark">
        <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="./images/optimal.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Designing Hierarchies for Optimal Hyperbolic Embedding</h3>
              <br>
              <strong>Melika Ayoughi</strong>, Max van Spengler, Pascal Mettes and Paul Groth
              <br>
              <em>Published at ESWC2025 conference</em>, 2025
              <br>
		    <a href="https://2025.eswc-conferences.org/research-resource-and-in-use-papers/">paper</a> / 
<!-- 		    <a href="https://arxiv.org/abs/2208.06662">arxiv</a> / -->
		    <a href="https://github.com/Melika-Ayoughi/Optimal-Hierarchy">code</a> /
<!-- 		    <a href="./data/.pdf">slides</a> / -->

              <p></p>
              <p>Hyperbolic geometry has shown to be highly effective for embedding hierarchical data structures. As such, machine learning in hyperbolic space is rapidly gaining traction across a wide range of disciplines, from recommender systems and graph networks to biological systems and computer vision.
The performance of hyperbolic learning commonly depends on the hierarchical information used as input or supervision. Given that knowledge graphs and ontologies are common sources of such hierarchies, this paper aims to guide ontology designers in designing hierarchies for use in these learning algorithms. Using widely employed measures of embedding quality with extensive experiments, we find that hierarchies are best suited for hyperbolic embeddings when they are wide, and single inheritance, independent of the hierarchy size and imbalance.</p>

            </td>
          </tr>
        </span>
        </tbody></table>
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <span class="dark">
        <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="./images/hyperclic.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Continual Hyperbolic Learning of Instances and Classes</h3>
              <br>
<!--               <strong>Melika Ayoughi</strong> , Pascal Mettes, Paul Groth -->
              <br>
              <em>Under review</em>, 2024
              <br>
<!-- 		    <a href="https://dl.acm.org/doi/10.1145/3583138">paper</a> /  -->
<!-- 		    <a href="./data/paper.pdf">paper</a> / -->
<!-- 		    <a href="https://github.com/Melika-Ayoughi/Self-Contained-Video-Entity-Discovery">code</a> / -->
<!-- 		    <a href="./data/.pdf">slides</a> / -->

              <p></p>
              <p>Instance-level continual learning addresses the challenging problem of recognizing and remembering specific instances of object classes in an incremental setup, where new instances appear over time. Continual learning of instances forms a more fine-grained challenge than conventional continual learning, which is only concerned with incremental discrimination at the class-level. In this paper, we argue that for real-world continual understanding, we need to recognize samples both at the instance- and class-level. We find that classes and instances form a hierarchical structure. To enable us to learn from this structure, we propose a hyperbolic continual learning algorithm for visual instances and classes. We introduce continual hyperbolic classification and distillation, allowing us to embed the hierarchical relations between classes and from classes to instances. Empirical evaluations show that our method can operate effectively at both levels of granularity and with better hierarchical generalization, outperforming well-known continual learning algorithms.</p>

            </td>
          </tr>
        </span>
        </tbody></table>
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <span class="dark">
        <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="./images/PART.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>PART: Self-supervised Pretraining with Pairwise Relative Translations</h3>
              <br>
<!--               <strong>Melika Ayoughi</strong> , Pascal Mettes, Paul Groth -->
              <br>
              <em>Under review</em>, 2024
              <br>
<!-- 		    <a href="https://dl.acm.org/doi/10.1145/3583138">paper</a> /  -->
		    <a href="./data/paper.pdf">paper</a> /
<!-- 		    <a href="https://github.com/Melika-Ayoughi/Self-Contained-Video-Entity-Discovery">code</a> / -->
<!-- 		    <a href="./data/.pdf">slides</a> / -->

              <p></p>
              <p>Images are often composed of objects and object parts that are related to each other but are not necessarily related to their absolute position in the image frame. For instance, the pose of a person's nose is consistent relative to the forehead, while that same nose can be anywhere in absolute position in the image frame. To capture these underlying relative relationships, we introduce PART, a novel pretraining approach that predicts pairwise relative translations between randomly sampled input patches. Through this process, the original patch positions are masked out. The pretraining objective is to predict the pairwise translation parameters for any set of patches, just using the patch content. Our object detection experiments on COCO show improved performance over strong baselines such as MAE and DropPos. Our method is competitive on the ImageNet-1k classification benchmark. Beyond vision, we also outperform baselines on 1D time series prediction tasks. The code and models will be available soon.</p>

            </td>
          </tr>
        </span>
        </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <span class="dark">
        <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="./images/method_.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Self-Contained Entity Discovery from Captioned Videos</h3>
              <br>
              <strong>Melika Ayoughi</strong> , Pascal Mettes, Paul Groth
              <br>
              <em>Published at TOMM journal</em>, 2022
              <br>
		    <a href="https://dl.acm.org/doi/10.1145/3583138">paper</a> / 
		    <a href="https://arxiv.org/abs/2208.06662">arxiv</a> /
		    <a href="https://github.com/Melika-Ayoughi/Self-Contained-Video-Entity-Discovery">code</a> /
		    <a href="./data/.pdf">slides</a> /

              <p></p>
              <p>This paper introduces the task of visual named entity discovery in videos without the need for task-specific supervision or task-specific external knowledge sources. Assigning specific names to entities (e.g. faces, scenes, or objects) in video frames is a long-standing challenge. Commonly, this problem is addressed as a supervised learning objective by manually annotating faces with entity labels. To bypass the annotation burden of this setup, several works have investigated the problem by utilizing external knowledge sources such as movie databases. While effective, such approaches do not work when task-specific knowledge sources are not provided and can only be applied to movies and TV series. In this work, we take the problem a step further and propose to discover entities in videos from videos and corresponding captions or subtitles. We introduce a three-stage method where we (i) create bipartite entity-name graphs from frame-caption pairs, (ii) find visual entity agreements, and (iii) refine the entity assignment through entity-level prototype construction. To tackle this new problem, we outline two new benchmarks SC-Friends and SC-BBT based on the Friends and Big Bang Theory TV series. Experiments on the benchmarks demonstrate the ability of our approach to discover which named entity belongs to which face or scene, with an accuracy close to a supervised oracle, just from the multimodal information present in videos. Additionally, our qualitative examples show the potential challenges of self-contained discovery of any visual entity for future work.</p>

            </td>
          </tr>
        </span>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Extra Curricular</heading>
            </td>
          </tr>
        <tr>
            <td><strong>October 2020-Now: </strong>Organization team of the <a href="https://uva-iai.github.io/"> inclusive AI program</a></td>

        </tr>
        </tbody></table>

        <table id="recent-activity-table" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Recent Activity</heading>
            </td>
          </tr>
        <tr>
		<tr><td><strong>May 2024: </strong>Presented our work at the <a href="https://sites.google.com/view/nccv2024/home">  Netherlands Conference on Computer Vision</a></td></tr>
		<tr><td><strong>August 2023: </strong>Joined  <a href="https://machinelearning.apple.com/">Apple MLR</a> for a 4-month internship.</td></tr>
		<tr><td><strong>January 2023: </strong>Our paper on  <a href="https://dl.acm.org/doi/10.1145/3583138"> "Self-Contained Entity Discovery from Captioned Videos"</a> got accepted at <a href="https://dl.acm.org/journal/tomm">TOMM journal </a></td></tr>
	    <tr><td><strong>November 2022: </strong>Presented our work at the <a href="https://wimlworkshop.org/2022-wiml-workshop/">  WiML at Neurips 2022 </a></td></tr>
            <tr><td><strong>July 2022: </strong>Presented our work at the <a href="https://cmp.felk.cvut.cz/summerschool2022/">  Vision and Sports Summer School </a></td></tr>
            <tr><td><strong>May 2022: </strong>Presented our work at the <a href="https://sites.google.com/view/nccv-2022">  Netherlands Conference on Computer Vision</a></td></tr>
        </tr>
        </tbody></table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
	<tr>
            <td><strong>Nov 2023: </strong>Master Thesis Supervision</td>
            
        </tr>
	<tr>
            <td><strong>Nov 2022: </strong>Master Thesis Supervision</td>
            
        </tr>
        <tr>
            <td><strong>April 2022: </strong>Bachelor Thesis Supervision</td>
            
        </tr>
	<tr>
            <td><strong>November 2022:</strong> Teaching Assistant Applied Machine Learning at UvA</td>
        </tr>
        <tr>
            <td><strong>November 2021:</strong> Teaching Assistant Applied Machine Learning at UvA</td>
        </tr>
        <tr>
            <td><strong>November 2020:</strong> Teaching Assistant Applied Machine Learning at UvA</td>
        </tr>
        <tr>
            <td><strong>September 2019:</strong> Teaching Assistant Machine Learning 1 at UvA</td
        </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
